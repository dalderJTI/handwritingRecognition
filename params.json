{"name":"Optical Character Recognition as an Input Method","tagline":"Applying Machine Learning to Recognize and Train on Written Characters","body":"![Intro Picture](https://raw.githubusercontent.com/dalderJTI/handwritingRecognition/develop/Thulika/ExampleImages/InputOutput.png)\r\n\r\n## Introduction\r\nHandwriting recognition has many useful applications. It can be used as user input for devices, or to decode and interpret pictures of words into editable unicode text. In the application for this project, I am using it as a means of user input as a replacement for the keyboard on Android devices. \r\n\r\n## Description\r\nTyping on a touchscreen isn't always the easiest. Without a physical keyboard, it is easy to hit the wrong keys or lose your hand position. On larger touch screens, when composing anything longer than a text, it may be more a fluid and easy process to write with a stylus rather than use a virtual keyboard. With the recent release of Google's Handwriting Input Engine, I became interested in how handwriting recognition works, and learns written characters based on user input. \r\n\r\nI found a project on GitHub called Thulika that I forked and began to learn in order to develop on. This application is developed for Android devices and uses the touch screen as the writing surface for the input method. It uses a neural network library named Encog which implements many different types and aspects of neural networks. The neural network chosen for this application was a Self Organizing Map.\r\n\r\nThis application was originally developed for the Malayalam language which uses rules that modify previous characters based on current input. It also recognizes and predicts only one character at a time. My attempt is to modify this to recognize words in the English language.\r\n\r\nWhen writing on the screen, the keyboard takes the image of the written character and translates it to a bitmap image. The black pixels making up the character are used as the data points for the SOM to map to, as described below.\r\n\r\n## Self Organizing Maps\r\n\r\nSelf Organizing Maps or SOMs, are a map of neurons that are thrown at a data plot. They employ competitive and unsupervised learning to predict the desired output. The competitive part means that the closest neuron to the data point on the plot is selected as the \"winner\" or Best Matching Unit (BMU). This neuron moves toward the data point, pulling its neighbors with it. This is repeated for however many iterations, and each time the map of neurons more closely resembles and represents the data points on the plot. \r\n\r\n![SOM](https://raw.githubusercontent.com/dalderJTI/handwritingRecognition/develop/Thulika/ExampleImages/SOMVisual.png)\r\n\r\nA good step by step description of how they work, as defined below were found  [here](http://www.ai-junkie.com/ann/som/som2.html), along with more information about SOMs and how they work and are used. \r\n\r\n1. Each node's weights are initialized.\r\n\r\n2. A vector is chosen at random from the set of training data and presented to the lattice.\r\n\r\n3. Every node is examined to calculate which one's weights are most like the input vector. The winning node is commonly known as the Best Matching Unit (BMU).\r\n\r\n4. The radius of the neighbourhood of the BMU is now calculated. This is a value that starts large, typically set to the 'radius' of the lattice,  but diminishes each time-step. Any nodes found within this radius are deemed to be inside the BMU's neighbourhood.\r\n\r\n5. Each neighbouring node's (the nodes found in step 4) weights are adjusted to make them more like the input vector. The closer a node is to the BMU, the more its weights get altered.\r\n\r\n6. Repeat step 2 for N iterations.\r\n\r\n## Supervised Error Correction\r\n\r\nSOMs themselves do a pretty good job at recognizing characters. Wouldn't it be better if instead of choosing or confirming each individual character after writing it, we could instead save the input upon completion and have the application choose a word that best matches it for us? Using the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm), this is possible. \r\n\r\nAfter each input character, the application returns a list of it's best guesses of what that input might be by using the SOM. This application will save that array and use it later. \r\n\r\n![Letter Prediction](https://raw.githubusercontent.com/dalderJTI/handwritingRecognition/develop/Thulika/ExampleImages/LetterPredictions.png)\r\n\r\nAs we accumulate these predicted character arrays, we store them into another array. When the 'space' button is pressed, signalling that the word is completed, we begin the processing.\r\n\r\n![Prediction Arrays](https://raw.githubusercontent.com/dalderJTI/handwritingRecognition/develop/Thulika/ExampleImages/PredictionArrays.jpg)\r\n\r\nSince our prediction arrays are already sorted by best guess, we can use the index as each prediction's score. According to the example picture, the predicted letters near the top will have the best scores. Using a dictionary to compare to, we then find a sequence of letters that match any word with the same length as our input array.\r\n\r\n![Letter Selection](https://raw.githubusercontent.com/dalderJTI/handwritingRecognition/develop/Thulika/ExampleImages/LetterSelection.png)\r\n\r\nWe then combine the scores for each letter in any confirmed matching word sequences and output them as a list to select from, sorted by score.\r\n\r\nWhen a word is selected, we are provided with a possible training example. For our example, the letter sequence of our selected word should match the letters across the tops of the prediction arrays. By passing the data mapping of the input and the confirmed letter it represents back to the SOM, we can provide it the information it needs to adjust it's future predictions. This is called supervised learning. ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}